{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "VLMresponses = pd.read_csv(\"/kaggle/input/vlmdata/VLM_responses_Baseline.csv\")\n",
    "VLMresponses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLMtrainResponses = pd.read_csv(\"/kaggle/input/vlmdata/VLM__training_responses.csv\")\n",
    "VLMtrainResponses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLMresponses = VLMresponses[VLMresponses[\"Accurate\"] != \"FALSE\"]\n",
    "VLMresponses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLMresponses = VLMresponses.drop(\"Why it Failed?\", axis='columns')\n",
    "VLMresponses = VLMresponses.reset_index(drop=True)\n",
    "VLMresponses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = VLMresponses.iloc[3][\"UserPrompt\"] + \"\\n\" + VLMresponses.iloc[3][\"AssistantResponse\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(VLMresponses)\n",
    "datasetTraining = Dataset.from_pandas(VLMtrainResponses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dataset)) \n",
    "print(dataset.features) \n",
    "print(datasetTraining.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Convert to Pandas DataFrame for easier splitting (if needed)\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Split into train and test sets (80%/20%)\n",
    "train_data = datasetTraining\n",
    "test_data = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_len=512, max_target_len=512):\n",
    "        self.inputs = data[\"AssistantResponse\"]\n",
    "        self.targets = data[\"SemanticParser\"]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = \"Instruction: \" + self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "\n",
    "        source = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_target_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": source[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "dataset = MyDataset(train_data, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)  # Slower LR helps stabilize\n",
    "\n",
    "epochs = 10   # More passes help low-data generalization\n",
    "patience = 3  # Stop early if no improvement\n",
    "\n",
    "# Optional: gradient accumulation if batch size is small\n",
    "grad_accum_steps = 2\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss / grad_accum_steps\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % grad_accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    print(f\"Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")  # Save best\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "print(\"Training done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # safer than eval\n",
    "\n",
    "# Assuming you have a DataFrame `vlm_responses_df` to collect the results\n",
    "parser_response = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for input_text in test_data[\"AssistantResponse\"]:\n",
    "    if not input_text:\n",
    "        parser_response.append(None)  # keep the length aligned\n",
    "        continue\n",
    "        \n",
    "    prompt = \"Convert this into the corresponding UI JSON:\\n: \" + str(input_text)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=256,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {input_text}\")\n",
    "    print(f\"Prediction: {output}\\n\")\n",
    "\n",
    "    try:\n",
    "        # Extract JSON-like part of the output\n",
    "        json_start = output.find(\"{\")\n",
    "        if json_start != -1:\n",
    "            json_part = output[json_start:]\n",
    "            parsed_output = ast.literal_eval(json_part)  # convert string to dict safely\n",
    "            parser_response.append(parsed_output)\n",
    "        else:\n",
    "            parser_response.append(output)  # or some fallback\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing output: {e}\")\n",
    "        parser_response.append(output)\n",
    "\n",
    "# Add to DataFrame\n",
    "VLMresponses[\"Seq2SeqParser\"] = parser_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLMresponses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLMresponses.to_csv(\"/kaggle/working/Seq2SeqParsingOutput.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
