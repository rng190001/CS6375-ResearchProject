{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "def load_and_clean_dataset():\n",
    "    dataset = load_dataset(\"ivelin/ui_refexp_saved\", split=\"train[:336]\")\n",
    "    # df = dataset.to_pandas()\n",
    "    # unique_df = df.drop_duplicates(subset=\"image_id\")\n",
    "    # cleaned_data = unique_df[\"image\"]\n",
    "    cleaned_data = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        # Check if entry is a string and attempt to convert to dict\n",
    "        if isinstance(entry, str):\n",
    "            try:\n",
    "                entry = json.loads(entry)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Failed to decode JSON string: {entry}\")\n",
    "                continue  # Skip this entry if decoding fails\n",
    "\n",
    "        cleaned_entry = {\n",
    "            \"image\": entry.get(\"image\"),  # Use get() to avoid KeyError if key is missing\n",
    "            \"image_id\": entry.get(\"image_id\")\n",
    "        }\n",
    "        cleaned_data.append(cleaned_entry)\n",
    "        \n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "prompts1 = pd.read_csv(\"/kaggle/input/prompts2/prompts - Sheet1.csv\")\n",
    "cleaned = load_and_clean_dataset()\n",
    "cleaned_df = pd.DataFrame(cleaned)\n",
    "cleaned_df = cleaned_df.drop_duplicates(subset=\"image_id\", keep=\"first\")\n",
    "cleaned_df= cleaned_df.drop(columns=['image_id'])\n",
    "assert len(cleaned_df) == len(prompts1), \"Lengths do not match!\"\n",
    "cleaned_df = cleaned_df.reset_index(drop=True)\n",
    "prompts1 = prompts1.reset_index(drop=True)\n",
    "cleaned_df[\"prompts\"] = prompts1\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(cleaned_df.iloc[99][\"image\"])\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.show()\n",
    "print(cleaned_df.iloc[99][\"prompts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.metadata\n",
    "print(importlib.metadata.version(\"bitsandbytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "# Set up 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Load model with quantization and device mapping\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image (ensure correct mode and size)\n",
    "# image = cleaned_df.iloc[8]['image']\n",
    "import pandas as pd\n",
    "\n",
    "VLMresponses = []\n",
    "\n",
    "for i in range(len(cleaned_df.index)):\n",
    "\n",
    "    image = cleaned_df.iloc[i]['image']\n",
    "    \n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    datasetPrompt = cleaned_df.iloc[i]['prompts']\n",
    "   \n",
    "    prompt = (\n",
    "        f\"USER: You are a virtual assistant helping a user interact with a mobile app. You are given only this screenshot of a mobile application.\\n\"\n",
    "        f\"<image>\\n\"\n",
    "        f\"Do not make assumptions beyond what is clearly visible in the image.\\n\"\n",
    "        f\"Identify all interactable UI elements shown, such as buttons,icons (share, close, menu, heart, search, etc.), text fields, checkboxes, sliders, and other actionable components.\\n\"\n",
    "        f\"The user wants to: '{datasetPrompt}'\\n\"\n",
    "        f\"Using only the current screenshot, return the shortest sequence of taps required to complete this task.\\n\"\n",
    "        f\"Only include necessary steps visible in the image. Limit it to 1-5 steps.\\n\"\n",
    "        f\"ASSISTANT:\"\n",
    "    )\n",
    "    \n",
    "    #f\"The task may require additional interactions with elements not shown on the current screen.\\n\"\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200, do_sample=True, top_p=0.9, temperature=0.7)\n",
    "    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    assistant_response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    VLMresponses.append({\n",
    "        \"Index\": i,\n",
    "        \"UserPrompt\": datasetPrompt,\n",
    "        \"AssistantResponse\": assistant_response\n",
    "    })\n",
    "    print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VLMresponses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame and save to CSV\n",
    "VLMres_df = pd.DataFrame(VLMresponses)\n",
    "VLMres_df.to_csv(\"/kaggle/working/VLM_responses.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
